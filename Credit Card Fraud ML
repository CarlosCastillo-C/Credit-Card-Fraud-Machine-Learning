---
title: "Credit Card Fraud Detection"
format: gfm
jupyter: python3
---

```{python}
import itertools
import math

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import (
    LabelEncoder,
    OneHotEncoder,
    StandardScaler
)
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from xgboost import XGBClassifier
from sklearn.model_selection import (
    ParameterGrid,
    StratifiedKFold,
    StratifiedShuffleSplit,
    train_test_split
)
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
    log_loss,
    r2_score,
    roc_auc_score,
    roc_curve,
    mean_squared_error
)
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler, SMOTENC
from plotnine import *
import missingno as msno
import shap
import dtreeviz
import statsmodels.api as sm
import statsmodels.formula.api as smf
from IPython.display import HTML, display
from stargazer.stargazer import Stargazer
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import shap

```

```{python}

data = pd.read_csv("~/Documents/Machine_Learning/transactions.csv")

```


```{python}
vis_1 = (
    ggplot(data, aes(x='merchant_category', y='is_fraud'))
    + geom_col()
    + geom_smooth()
    + theme_bw()
    + theme(
        panel_grid_major=element_blank(),
        panel_grid_minor=element_blank(),
        panel_border=element_blank(),
        panel_background=element_blank(),
        figure_size=(10, 8)
    )
)

vis_1


```

```{python}
vis_2 = (
    ggplot(data, aes(x='three_ds_flag', y='is_fraud'))
    + geom_col()
    + geom_smooth()
    + theme_bw()
    + theme(
        panel_grid_major=element_blank(),
        panel_grid_minor=element_blank(),
        panel_border=element_blank(),
        panel_background=element_blank(),
        figure_size=(10, 8)
    )
)

vis_2
```

```{python}
data[['avs_match', 'is_fraud']].value_counts()

```

```{python}
NAOF = len(data[data.is_fraud == 0]) ## No Amount of Fraud
AOF = len(data[data.is_fraud == 1])   ## Amount of Fraud

print(NAOF)
print(AOF)

POF = round(AOF/NAOF * 100, 2)## % of Fraud

print(f'{POF}%')
```

```{python}

labels = ['NAOF', 'AOF']
count_labels = data.value_counts(data['is_fraud'], sort= True)
count_labels.plot(kind = "bar", rot = 0)
plt.ylabel("Count")
plt.xticks(range(2), labels)
plt.show()

```

```{python}
##Scale data
scaler = StandardScaler()
scaled_data = data.copy()
scaled_data.drop(['transaction_id', 'user_id'], inplace = True, axis=1)

```


```{python}

y_scaled = scaled_data['is_fraud']
X_scaled = scaled_data.drop(['is_fraud'], axis =1)

```


```{python}
cat_cols = X_scaled.select_dtypes(include=["object", "category"]).columns.tolist()
num_cols = X_scaled.select_dtypes(include=[np.number]).columns.tolist()

```


Logit Regression

```{python}
Logit_data = scaled_data.copy()
y_logit = Logit_data['is_fraud']
x_logit = sm.add_constant(Logit_data[num_cols])

fit_10 = sm.Logit(y_logit, x_logit).fit(disp=False)
print(fit_10.summary2())

```

```{python}

cat_cols_new = cat_cols.copy()

cat_cols_new.remove('transaction_time')

```

```{python}
X_cat = pd.get_dummies(Logit_data[cat_cols_new], drop_first=True)

```

```{python}

X_final_logit = pd.concat([x_logit, X_cat], axis=1)
X_final_logit = sm.add_constant(X_final_logit)

logit_df = pd.concat([y_logit, X_final_logit], axis=1).dropna()

y_clean = pd.to_numeric(logit_df['is_fraud'], errors='raise')
X_clean = logit_df.drop(columns=['is_fraud']).apply(
    pd.to_numeric, errors='raise'
)

fit_safe = sm.Logit(y_clean.astype(float), X_clean.astype(float)).fit(disp=False)
print(fit_safe.summary2())


```

Decision Tree below

```{python}

# Build explicit categories per categorical column
cats_map = {c: sorted(X_scaled[c].dropna().unique().tolist()) for c in cat_cols}
cat_cols_eff = [c for c, cats in cats_map.items() if len(cats) > 0]
ohe_categories = [cats_map[c] for c in cat_cols_eff]

```

```{python}

# Preprocess: one-hot encode categoricals and impute missing values
preprocess = ColumnTransformer(
    transformers=[
        ("cat",
         OneHotEncoder(
             categories=ohe_categories,
             handle_unknown="ignore",
             sparse_output=False,
             drop=None  # keep both dummies for binary so NaN -> [0,0]
         ),
         cat_cols_eff),
        ("num",
         Pipeline([("imputer", SimpleImputer(strategy="median"))]),
         num_cols),
    ],
    remainder="drop",
)

tree_clf = DecisionTreeClassifier(random_state=123,
                                  max_depth=3) # Initalize model

```

```{python}

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_scaled, test_size=0.2, stratify=y_scaled, random_state=123
)

```

```{python}

preprocess = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), cat_cols),
        ('num', 'passthrough', num_cols)
    ]
)

tree_clf = DecisionTreeClassifier(random_state=123,
                                  max_depth=3)
clf = Pipeline(steps=[('preprocess', preprocess), ('model', tree_clf)])

clf.fit(X_train, y_train)

```

```{python}

predictions_dt = clf.predict(X_test)
decision_tree_score = clf.score(X_test, y_test) * 100

print("Decision Tree accuracy (%):", decision_tree_score)

```

```{python}

X_trans = preprocess.fit_transform(X_scaled)

```

```{python}

print(confusion_matrix(y_test, predictions_dt))
print(classification_report(y_test, predictions_dt))

```


```{python}

def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()


```


```{python}

confusion_matrix_dt = confusion_matrix(y_test, predictions_dt.round())
print("Confusion Matrix - Decision Tree")
print(confusion_matrix_dt)

```



```{python}

plot_confusion_matrix(confusion_matrix_dt, classes=[0, 1], title= "Confusion Matrix - Decision Tree")

```


```{python}
print(58472/(58472+145))
print((755/(567+755)))
print((58472/(58472+145) + (567/(567+755)))/2)

```

Xgboost

```{python}

cat_cols_again = ['country','bin_country', 'channel', 'merchant_category']

# one-hot encode
X_train_enc = pd.get_dummies(X_train_XGB, columns=cat_cols_again)
X_test_enc  = pd.get_dummies(X_test_XGB,  columns=cat_cols_again)

```


```{python}

# Set up XGBDmatrix
dtrain = xgb.DMatrix(data=X_train_enc.values, label=y_train_XGB)
dtest  = xgb.DMatrix(data=X_test_enc.values,  label=y_test_XGB)

```


```{python}

params = {
        "objective": "binary:logistic", # Set objective
        "eval_metric": ["auc", "error"],  # Track both AUC and error
        "seed": 42, # set seed

    }
num_boost_round = 300 # Set number of rounds

```


```{python}

watchlist = [(dtrain, "train")] # Set data for evaluation
booster = xgb.train(params, # Set parameters
                    dtrain,  # Set training data
                    num_boost_round=num_boost_round, # Set number of rounds
                    evals=watchlist,  # Set data to evaluate on
                    verbose_eval=50) # Set print out frequency

```


```{python}

test_pred_raw = booster.predict(dtest) # Create predictions
test_pred_raw

```


```{python}

test_pred_cls = (test_pred_raw >= 0.5).astype(int)

```


```{python}

print("\nConfusion matrix:")
cm = (confusion_matrix(y_test_XGB, test_pred_cls))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)# Set class labels
disp.plot(cmap="Blues") # Set color map
plt.title("Confusion Matrix — XGBoost") # Set title
plt.show() # Display plot
print("\nAccuracy):")
print(accuracy_score(y_test_XGB, test_pred_cls)) # Get classification report

```

Tuning XgBoost

```{python}

params = {
    "objective": "binary:logistic",   # Set objective
    "eta": 0.1,                       # Set learning rate
    "eval_metric": ["auc", "error"],  # Track both AUC and error
    "tree_method": "hist",
    "seed": 111111,
    "nthread": 1,                     # Parallel threads
}

# Run CV inside XGBoost
cv_res = xgb.cv(
    params=params,
    dtrain=dtrain,              # Training data (DMatrix)
    num_boost_round=400,       # Number of rounds
    nfold=5,                    # 5-fold CV
    verbose_eval=20,            # Print every 20 iters
    stratified=True,            # Good practice for classification
    shuffle=True,
)

```


```{python}

best_idx = cv_res['test-error-mean'].idxmin()
best_iter = int(best_idx) + 1 # Increment by 1 to get iteration
best_err  = float(cv_res.loc[best_idx, 'test-error-mean']) # Extract test error
best_auc  = float(cv_res.loc[best_idx, 'test-auc-mean']) if 'test-auc-mean' in cv_res.columns else np.nan # Extract test AUC

# Print results
print(f"Best iteration (by min test error): {best_iter}")
print(f"Min test error at best iter: {best_err:.6f}")
if not np.isnan(best_auc):
    print(f"Test AUC at best iter: {best_auc:.6f}")

```


```{python}

df_plot = cv_res.reset_index().rename(columns={"index": "iter"})
df_plot["iter"] = df_plot["iter"] + 1  # 1-based

# Safe column names (hyphens -> underscores) and ribbon bounds
df_plot = df_plot.rename(columns=lambda c: c.replace("-", "_"))
df_plot["test_error_lower"] = df_plot["test_error_mean"] - df_plot["test_error_std"]
df_plot["test_error_upper"] = df_plot["test_error_mean"] + df_plot["test_error_std"]

# Create plot
p_err = (
    ggplot(df_plot, # Set data
           aes(x="iter", y="test_error_mean")) # Set X and Y
    + geom_line() # Set line
    + geom_ribbon(aes(ymin="test_error_lower", ymax="test_error_upper"), alpha=0.15) # Set error bounds
    + geom_vline(xintercept=best_iter, linetype="dashed", alpha=0.6) # Add vertical line at best iteration
    + theme_bw() # Set theme
    + labs( # Set labels
        title="XGBoost CV (no early stopping): Iterations vs Validation Error",
        x="Boosting iteration",
        y="Test error (mean ± 1 sd across folds)"
    )
)
# View plot
p_err

```


```{python}

df_long = df_plot.melt( # Melt data
    id_vars=["iter"],
    value_vars=["train_error_mean", "test_error_mean"],
    var_name="series",
    value_name="err"
)
# Create plot
p_err_both = (
    ggplot(df_long, # Set data
           aes(x="iter", y="err", color="series")) # Set aesthetics
    + geom_line(alpha=0.9) # Set geom line
    + geom_vline(xintercept=best_iter, linetype="dashed", alpha=0.6) # Add vertical line for best iteration
    + theme_bw() # Set theme
    + labs( # Set labels
        title="Train vs Test Error over Boosting Iterations (no ES)",
        x="Boosting iteration",
        y="Error"
    )
)
# Display plot
p_err_both

```


```{python}

# Set range of parameter values to try
grid = {
    "max_depth": [3,  7, 10],
    "min_child_weight": [ 5, 7, 10],
}
param_grid = list(ParameterGrid(grid))

# Set base model parameters
base_params = {
    "objective": "binary:logistic",
    "eta": 0.10,
    "eval_metric": ["error", "auc"],
    "tree_method": "hist",
    "seed": 111111,
    "nthread": 1,  # keep each worker single-threaded to avoid oversubscription
}

```


```{python}

def run_one_cv(md, mcw):
    """Run xgb.cv for a single (max_depth, min_child_weight) pair and return best metrics."""
    params = base_params.copy()
    params.update({"max_depth": int(md), "min_child_weight": int(mcw)})

    cv = xgb.cv(
        params=params,
        dtrain=dtrain,              # DMatrix from earlier
        num_boost_round=1000,        # nrounds = 100
        nfold=5,                    # 5-fold CV
        early_stopping_rounds=20,   # stop if no improvement
        stratified=True,
        shuffle=True,
        verbose_eval=False,
        seed=111111,
    )

    # Best round is the length of the early-stopped trace
    best_round = len(cv)

    # Read AUC & error at the best round row explicitly
    best_row = cv.iloc[best_round - 1]
    best_err = float(best_row["test-error-mean"])
    best_auc = float(best_row["test-auc-mean"])

    # Return results
    return {
        "max_depth": md,
        "min_child_weight": mcw,
        "best_round": best_round,
        "test_error": best_err,
        "test_auc": best_auc,
    }


```


```{python}

results = []
for p in tqdm(param_grid, desc="Grid CV (serial)"):
    results.append(run_one_cv(p["max_depth"], p["min_child_weight"]))

```


```{python}

# Create and sort results data frame
cv_results_df = (
    pd.DataFrame(results)
      .sort_values(["test_error", "test_auc"], ascending=[True, False])
      .reset_index(drop=True)
)

# Identify best parameters0
best_pair = cv_results_df.iloc[0].to_dict()
print("Best (by min test_error, then max AUC):", best_pair)

```


```{python}

# Create results data frame
res_db = (
    cv_results_df[["max_depth", "min_child_weight", "test_auc", "test_error"]]
    .rename(columns={"test_auc": "auc", "test_error": "error"})
    .copy()
)


# Convert to categorical (factor) for plotting axes
res_db["max_depth"] = res_db["max_depth"].astype("category")
res_db["min_child_weight"] = res_db["min_child_weight"].astype("category")


# Compute midpoints for diverging color scales
auc_midpoint = res_db["auc"].mean()
err_midpoint = res_db["error"].mean()

```

```{python}

g_2 = (
    ggplot(res_db, aes(y="max_depth", x="min_child_weight", fill="auc"))  # set aesthetics
    + geom_tile()                                                         # Use geom_tile for heatmap
    + theme_bw()                                                          # Set theme
    + scale_fill_gradient2(
        low="blue", mid="white", high="red",                              # Choose colors
        midpoint=auc_midpoint,                                            # Choose mid point
        na_value="grey",                                                  # Choose NA value

    )
    + labs(x="Minimum Child Weight", y="Max Depth", fill="AUC")           # Set labels
)
g_2

```

```{python}

g_3 = (
    ggplot(res_db, aes(y="max_depth", x="min_child_weight", fill="error"))  # set aesthetics
    + geom_tile()                                                            # Use geom_tile for heatmap
    + theme_bw()                                                             # Set theme
    + scale_fill_gradient2(
        low="blue", mid="white", high="red",                                 # Choose colors
        midpoint=err_midpoint,                                               # Choose mid point
        na_value="grey",                                                     # Choose NA values
    )
    + labs(x="Minimum Child Weight", y="Max Depth", fill="Error")            # Set labels
)
display(g_3)

```

```{python}

tuned_max_depth = int(best_pair['max_depth']) # Extract max depth
tuned_min_child = int(best_pair['min_child_weight']) # Extract min_child_weight

```


```{python}

# Define gamma grid
gamma_vals = [0.00, 0.05, 0.10, 0.15, 0.20]

# Set base parameters
base_params = {
    "objective": "binary:logistic",
    "eta": 0.10,
    "max_depth": tuned_max_depth, # Uuse tuned value of max depth
    "min_child_weight": tuned_min_child, # Use tuned value of min child weight
    "tree_method": "hist",
    "eval_metric": ["auc", "error"],
    "seed": 111111,
    "nthread": 1,
}

```


```{python}

### Be careful this can take a long time to run ###
rows = [] # Create data frame to store valus
for g in tqdm(gamma_vals, desc="Gamma CV (serial)"): # For each gamma value
    params = base_params.copy() # Create copy of base parameters
    params["gamma"] = float(g) # Replace value with current gamma value

    # Run xgb.cv
    cv = xgb.cv(
        params=params,
        dtrain=dtrain,                # Set training data
        num_boost_round=100,          # Set number of rounds
        nfold=5,                      # Set folds for cross validation
        early_stopping_rounds=20,     # Set early stopping rounds
        stratified=True,
        shuffle=True,
        verbose_eval=False,
        seed=111111, #Set seed
    )

    # Best iteration is the length of the early-stopped trace
    best_round = len(cv)
    best_row = cv.iloc[best_round - 1]
    # Store results from current iteration
    rows.append({
        "gamma": g,
        "best_round": int(best_round),
        "test_auc": float(best_row["test-auc-mean"]),
        "test_error": float(best_row["test-error-mean"]),
    })

# Join results into data frame
gamma_results = (pd.DataFrame(rows)
                   .sort_values(['test_error', 'test_auc'], ascending=[True, False])
                   .reset_index(drop=True))
# View results
display(gamma_results)
# Extract best value
best_gamma = float(gamma_results.iloc[0]['gamma'])
# Print out vest value
print(f"Selected gamma (by min test_error, then max AUC): {best_gamma:.2f}")

```


```{python}

tuned_gamma = float(best_gamma) # Extract best gamma value

```


```{python}

# Create grid of possible values
grid = {
    "subsample":        [0.6, 0.7, 0.8, 0.9, 1.0],
    "colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],
}
# Convert into parameter grid and then list
param_grid = list(ParameterGrid(grid))
# Set base parameters
base_params = {
    "objective": "binary:logistic",
    "eta": 0.10,
    "max_depth": tuned_max_depth, # Use tuned value for max depth
    "min_child_weight": tuned_min_child, # Use tuned value for min child weight
    "gamma": tuned_gamma, # Use tuned value for gamma
    "tree_method": "hist",
    "eval_metric": ["auc", "error"],
    "seed": 111111,
    "nthread": 1,                     # single core
}

```


```{python}

# Create function
def run_one_cv(subsample, colsample_bytree):
    """Run xgb.cv for a single (subsample, colsample_bytree) and return best metrics."""
    params = base_params.copy() # Create copy of base parameters
    params.update({ # Update with values of subsample and colsample_bytree
        "subsample": float(subsample),
        "colsample_bytree": float(colsample_bytree),
    })
    # Run xgb.cv
    cv = xgb.cv(
        params=params,  # Set parameters
        dtrain=dtrain,  # Set training data
        num_boost_round=100, # Set number of rounds
        nfold=5,  # Set cross-validation folds
        early_stopping_rounds=20, # Set number of early stopping rounds
        stratified=True,
        shuffle=True,
        verbose_eval=False,
        seed=111111, # Set seed
    )

    best_round = len(cv)             # early-stopped length
    best_row = cv.iloc[best_round - 1] # Identify best row
    # Return results
    return {
        "subsample": subsample,
        "colsample_bytree": colsample_bytree,
        "best_round": int(best_round),
        "test_auc": float(best_row["test-auc-mean"]),
        "test_error": float(best_row["test-error-mean"]),
    }

```


```{python}

### Be careful this can take a long time to run ###
rows = [] # Create empty list to store results
# For each set of parameters
for p in tqdm(param_grid, desc="Subsample × Colsample_bytree CV (serial)"):
    rows.append(run_one_cv(p["subsample"], p["colsample_bytree"])) # Run tuning function and store results
# Convert results into data frame
sc_results = (pd.DataFrame(rows)
                .sort_values(['test_error','test_auc'], ascending=[True, False])
                .reset_index(drop=True))
# View results
display(sc_results.head(10))
# Identify best results
best_sc = sc_results.iloc[0].to_dict()
# Store best results
print(f"Selected subsample={best_sc['subsample']}, "
      f"colsample_bytree={best_sc['colsample_bytree']} "
      f"(min test_error={best_sc['test_error']:.6f}, AUC={best_sc['test_auc']:.6f}, "
      f"best_round={best_sc['best_round']})")

```

```{python}

# Convert treults table to have simpler names and drop round
res_db = (
    sc_results[["subsample", "colsample_bytree", "test_auc", "test_error"]]
    .rename(columns={
        "colsample_bytree": "colsample_by_tree",
        "test_auc": "auc",
        "test_error": "error"
    })
    .copy()
)

res_db["subsample"] = res_db["subsample"].astype("category")            # Convert tree number to factor for plotting
res_db["colsample_by_tree"] = res_db["colsample_by_tree"].astype("category")  # Convert node size to factor for plotting


auc_midpoint = res_db["auc"].mean() # Calculate mean AUC
err_midpoint = res_db["error"].mean() # Calculate mean Error

```


```{python}

# Create plot
g_4 = (
    ggplot(res_db, aes(y="colsample_by_tree", x="subsample", fill="auc"))  # set aesthetics
    + geom_tile()                                                          # Use geom_tile for heatmap
    + theme_bw()                                                           # Set theme
    + scale_fill_gradient2(
        low="blue", mid="white", high="red",                               # Choose colors
        midpoint=auc_midpoint,                                             # Choose mid point
        na_value="grey",                                                   # Choose NA value
        guide="colorbar"                                                   # Set color bar (plotnine uses 'colorbar')
    )
    + labs(x="Subsample", y="Column Sample by Tree", fill="AUC")           # Set labels
)
display(g_4)  # Generate plot

```


```{python}

# Create plot
g_5 = (
    ggplot(res_db, aes(y="colsample_by_tree", x="subsample", fill="error"))  # set aesthetics
    + geom_tile()                                                            # Use geom_tile for heatmap
    + theme_bw()                                                             # Set theme
    + scale_fill_gradient2(
        low="blue", mid="white", high="red",                                 # Choose colors
        midpoint=err_midpoint,                                               # Choose mid point
        na_value="grey",                                                     # Choose NA value
        guide="colorbar"                                                     # Set color bar
    )
    + labs(x="Subsample", y="Column Sample by Tree", fill="Error")           # Set labels
)
display(g_5)  # Generate plot

```


```{python}

tuned_subsample = float(best_sc['subsample']) # Extract best subsample
tuned_colsample = float(best_sc['colsample_bytree']) # Extract best colsample_bytree

```


```{python}

# Set ETA values to try
etas = [0.3, 0.1, 0.05, 0.01, 0.005]
# Set base parameters
base_params = {
    "objective": "binary:logistic",
    "eval_metric": ["auc", "error"],
    "max_depth": tuned_max_depth, # Use tuned value for max depth
    "min_child_weight": tuned_min_child, # Use tuned value for min_child_weight
    "gamma": tuned_gamma, # Use tuned value for gamma
    "subsample": tuned_subsample, # Use tuned value for subsample
    "colsample_bytree": tuned_colsample, # Use tuned value for colsample_bytree
    "tree_method": "hist",
    "seed": 111111,
    "nthread": 1,                  # single core
}



```


```{python}

### Be careful this can take a long time to run ###
curves = []     # per-iteration logs for plotting
summaries = []  # one row per eta
# For each learning rate
for eta in tqdm(etas, desc="Learning-rate CV (serial)"):
    params = base_params.copy() # Create copy of parmameters
    params["eta"] = float(eta) # Update ETA value
    # Apply xgb.cv
    cv = xgb.cv(
        params=params, # Set parameters
        dtrain=dtrain, # Set training data
        num_boost_round=1000,  # run to 1000 unless ES stops early
        nfold=5, # Set folds for cross validation
        early_stopping_rounds=100, # Set early stopping rounds
        stratified=True,
        shuffle=True,
        verbose_eval=False,
        seed=111111,
    )

    # Extract data for model performance
    df_log = cv.reset_index().rename(columns={"index": "iter"})
    df_log["iter"] = df_log["iter"] + 1 # Increment iterations to get real number
    # fix hyphenated column names for plotnine
    df_log = df_log.rename(columns=lambda c: c.replace("-", "_"))
    df_log["eta"] = str(eta) # Store ETA value as a string
    curves.append(df_log) # Add values to data store

    # Identify best iteration
    best_round = len(cv)
    best_row = cv.iloc[best_round - 1] # Identify best row


    best_err = float(best_row["test-error-mean"]) # Extract best error value
    best_auc = float(best_row["test-auc-mean"]) # Extract best AUC value
    # Store results
    summaries.append({"eta": eta, "best_round": best_round, "test_error": best_err, "test_auc": best_auc})

# Combine curve data
curves_df = pd.concat(curves, ignore_index=True)
# Create data frame of result data
summ_df = pd.DataFrame(summaries).sort_values(
    ["test_error","test_auc"] ,
    ascending=[True, False]
).reset_index(drop=True)

display(summ_df.head())

```


```{python}

# Precompute ribbons (mean ± 1 sd) per-eta
curves_df["y_lower"] = curves_df["test_error_mean"] - curves_df["test_error_std"]
curves_df["y_upper"] = curves_df["test_error_mean"] + curves_df["test_error_std"]

# Create plot
p_lr = (
    ggplot(curves_df,  # Set dataset
           aes(x="iter", y="test_error_mean", color="eta", group="eta")) # Set Aesthetics
    + geom_line() # Set geom line
    + geom_ribbon(aes(ymin="y_lower", ymax="y_upper", fill="eta"), linetype="dashed", alpha=0.1, color="grey") # Add error bars
    + theme_bw() # Set theme
    + labs( # Set labels
        title="Learning Curves: Iterations vs Validation Error by Learning Rate",
        x="Boosting iteration",
        y=("Test error (mean ± 1 sd)" ),
        color="eta", fill="eta"
    )
)
# Display plot
display(p_lr)

```


```{python}

best_eta = float(summ_df.iloc[0]["eta"]) # Extract best learning rate
best_round = int(summ_df.iloc[0]["best_round"]) # Extract best round
print(f"Selected eta={best_eta} with best_round={best_round}, " # Print results
      f"test_error={summ_df.iloc[0]['test_error']:.6f}, "
      f"AUC={summ_df.iloc[0]['test_auc']:.6f}")

```


```{python}

# Set tuned parameters
tuned_max_depth = 3
tuned_min_child = 7
tuned_gamma = 0
tuned_subsample = 1
tuned_colsample = 1
tuned_eta = 0.05

```


```{python}

params = {
    "objective": "binary:logistic",
    "eval_metric": ["auc", "error"],
    "max_depth": tuned_max_depth, # Use tuned value for max depth
    "min_child_weight": tuned_min_child, # Use tuned value for min_child_weight
    "gamma": tuned_gamma, # Use tuned value for gamma
    "subsample": tuned_subsample, # Use tuned value for subsample
    "colsample_bytree": tuned_colsample, # Use tuned value for colsample_bytree
    "eta": tuned_eta, # Use tuned value for eta
    "tree_method": "hist",
    "seed": 111111,
    "nthread": 1,                  # single core
}
# Set up XGBoost
watchlist = [(dtrain, "train")] # Set data for evaluation
# Apply model
xgb_tuned = xgb.train(params, # Set parameters
                    dtrain,  # Set training data
                    num_boost_round=203, # Set number of rounds to previous best round
                    evals=watchlist,  # Set data to evaluate on
                    verbose_eval=50) # Set print out frequency

```


```{python}

test_pred_tuned = xgb_tuned.predict(dtest) # Create predictions


# Convert predictions into classes at 0.5
test_pred_cls_t = (test_pred_tuned >= 0.5).astype(int)


print("\nConfusion matrix:")
cm = (confusion_matrix(y_test_XGB, test_pred_cls_t))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)# Set class labels
disp.plot(cmap="Blues") # Set color map
plt.title("Confusion Matrix — Tuned XGBoost") # Set title
plt.show() # Display plot
print("\nAccuracy):")
print(accuracy_score(y_test_XGB, test_pred_cls_t)) # Get Accuracy

```

Add weights

```{python}

# Count values
counts = pd.Series(y_train_XGB).value_counts().sort_index()
neg = int(counts.get(0, 0)); pos = int(counts.get(1, 0)) # Calculate positive and negative samples
print(f"Number of negative samples: {neg}")
print(f"Number of positive samples: {pos}")

```

```{python}

# Calculate ratio
ratio = neg / pos
# Set ratio as weight for positive samples
w_tr = np.where(y_train == 1, ratio, 1.0).astype(np.float32)
# View weight vector
w_tr

```


```{python}

cat_cols_again = ['country','bin_country', 'channel', 'merchant_category']

# one-hot encode
X_train_enc = pd.get_dummies(X_train_XGB, columns=cat_cols_again)
X_test_enc  = pd.get_dummies(X_test_XGB,  columns=cat_cols_again)

```

```{python}

# Build weighted DMatrices
dtrain_w = xgb.DMatrix(X_train_enc.values, label=y_train_XGB, weight=w_tr)

```


```{python}

watchlist = [(dtrain_w, "train")] # Set data for evaluation
xgb_w = xgb.train(params, # Set parameters
                    dtrain_w,  # Set training data
                    num_boost_round=207, # Set number of rounds
                    evals=watchlist,  # Set data to evaluate on
                    verbose_eval=50) # Set print out frequency

```

```{python}

test_pred_w = xgb_w.predict(dtest) # Create predictions


# Convert predictions into classes at 0.5
test_pred_cls_w = (test_pred_w >= 0.5).astype(int)


print("\nConfusion matrix:")
cm = (confusion_matrix(y_test_XGB, test_pred_cls_w))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)# Set class labels
disp.plot(cmap="Blues") # Set color map
plt.title("Confusion Matrix — Weighted XGBoost") # Set title
plt.show() # Display plot
print("\nAccuracy):")
print(accuracy_score(y_test_XGB, test_pred_cls_w)) # Get Accuracy
print("\nAccuracy):")
print(accuracy_score(y_test_XGB, test_pred_cls_w)) # Get Accuracy

```

ROC Curves

```{python}

pred_dict = {}
pred_dict['Raw']      = np.asarray(test_pred_raw).ravel()
pred_dict['Tuned']    = np.asarray(test_pred_tuned).ravel()
pred_dict['Weighted'] = np.asarray(test_pred_w).ravel()


# Build ROC curves & AUCs
curves = [] # Create empty files to store results
aucs = {}
# For each set of predictions
for name, preds in pred_dict.items():
    fpr, tpr, _ = roc_curve(y_test_XGB, preds) # Calculate ROC
    auc_val = roc_auc_score(y_test_XGB, preds) # Calculate AUC
    aucs[name] = auc_val # Store AUC
    curves.append(pd.DataFrame({"fpr": fpr, "tpr": tpr, "Model": name})) # Store ROC

# Joine results to data frame
df_curves = pd.concat(curves, ignore_index=True)
# Put AUC in the legend label
df_curves["Label"] = df_curves["Model"].map(lambda m: f"{m} (AUC={aucs[m]:.3f})")

# Create plot
p_roc = (
    ggplot(df_curves,  # Set data frame
           aes(x="fpr", y="tpr", color="Label")) # Set aesthetics
    + geom_line(size=1.05, alpha=0.95) # Set geom_line
    + geom_abline(intercept=0, slope=1, linetype="dashed", alpha=0.6) # Set 45 degree line
    + coord_equal()
    + theme_classic()
    + theme(
        legend_position="bottom",
        legend_title=element_text(size=9),
        legend_text=element_text(size=9),
        axis_text=element_text(size=9),
        axis_title=element_text(size=10)
    )
    + labs( # Set labels
        title="ROC Curves on Test Set",
        x="False Positive Rate",
        y="True Positive Rate",
        color=""  # hide legend title
    )
)
# Generate plot
p_roc


```

Get feature importances

```{python}

# Get list of features
feature_cols = [c for c in list(X_train_enc.columns[:220]) if c != 'class']

# Map model's internal feature codes (f0, f1, ...) back to column names
feat_map = {f"f{i}": feature_cols[i] for i in range(len(feature_cols))}

```


```{python}

# Extract importance by different definitions
imp_gain       = xgb_tuned.get_score(importance_type='gain')        # Average gain when used
imp_weight     = xgb_tuned.get_score(importance_type='weight')      # Number of times used in splits
imp_cover      = xgb_tuned.get_score(importance_type='cover')       # Average coverage of splits
imp_total_gain = xgb_tuned.get_score(importance_type='total_gain')  # Sum of gain across splits
imp_total_cov  = xgb_tuned.get_score(importance_type='total_cover') # Sum of coverage across splits

# Convert to tidy DataFrame and join all importance types
def to_df(d, kind):
    if not d:
        return pd.DataFrame(columns=['feature_code','feature','kind','value'])
    df = (pd.Series(d).rename_axis('feature_code')
                      .reset_index(name='value'))
    df['feature'] = df['feature_code'].map(feat_map).fillna(df['feature_code'])
    df['kind'] = kind
    return df

# Join data
imp_long = pd.concat([
    to_df(imp_gain, 'gain'),
    to_df(imp_weight, 'weight'),
    to_df(imp_cover, 'cover'),
    to_df(imp_total_gain, 'total_gain'),
    to_df(imp_total_cov, 'total_cover'),
], ignore_index=True)

# View imp_gain
imp_gain

```


```{python}

imp_wide = (imp_long # Create data frame in long form
            .pivot_table(index=['feature_code','feature'], columns='kind', values='value', aggfunc='first')
            .reset_index())
# Create total sum
imp_wide['total_sum'] = imp_wide['total_gain'].fillna(0) + imp_wide['total_cover'].fillna(0)
# View results
imp_wide.sort_values('total_sum', ascending=False).head()

```


```{python}

imp_wide = (imp_long # Create data frame to store results
            .pivot_table(index=['feature_code','feature'], columns='kind', values='value', aggfunc='first')
            .reset_index())
scaler1 = StandardScaler() # Set up scalers
scaler2 = StandardScaler() # Set up scalers

```


```{python}

imp_wide = (imp_long # Create data frame to store results
            .pivot_table(index=['feature_code','feature'], columns='kind', values='value', aggfunc='first')
            .reset_index())
scaler1 = StandardScaler() # Set up scalers
scaler2 = StandardScaler() # Set up scalers
# Create total sum
imp_wide['total_sum'] = scaler1.fit_transform(imp_wide['total_gain'].values.reshape(-1, 1)) + scaler2.fit_transform(imp_wide['total_cover'].values.reshape(-1, 1))
# Sort and display values
imp_wide.sort_values('total_sum', ascending=False).head()

```


```{python}

# Extract top 15 features
top15_feats = (imp_wide.sort_values('total_sum', ascending=False)
                        .head(15)['feature'].tolist())
# View top 15 features
top15_feats

```


```{python}

# Convert to long form for plotting
plot_long = pd.concat([
    imp_long[imp_long['kind'].isin(['weight','total_gain','total_cover'])],
    imp_wide[['feature_code','feature','total_sum']].assign(kind='total_sum').rename(columns={'total_sum':'value'})
], ignore_index=True)
# Extract top 15
plot_long = plot_long[plot_long['feature'].isin(top15_feats)].copy()
# View results
plot_long

```


```{python}

# Order features by total_sum so all facets share a sensible order
order = (imp_wide.set_index('feature').loc[top15_feats, 'total_sum']
                .sort_values(ascending=True).index.tolist())  # ascending for nice top-down after coord_flip
plot_long['feature'] = pd.Categorical(plot_long['feature'], categories=order, ordered=True)


```


```{python}

# Convert data to long form for plotting
plot_long['metric'] = plot_long['kind'].map({
    'weight': 'Weight (split count)',
    'total_gain': 'Total Gain',
    'total_cover': 'Total Cover',
    'total_sum': 'Total Gain + Total Cover'
}).fillna(plot_long['kind'])
# Convert feature to categorical
plot_long['feature'] = plot_long['feature'].astype('category')
# View data
plot_long.head()

```


```{python}

p_imp = (
    ggplot(plot_long, aes(y='value', x='feature', fill='feature'))   # set aesthetics
    + geom_col(alpha=0.85)                                           # bars
    + coord_flip()                                                   # horizontal bars
    + facet_wrap('~metric', scales='free_x', ncol=2)                 # panels
    + scale_fill_hue()
    + theme_classic()                                                # removes background grid
    + theme(
        legend_position='none',                                      # hide color legend
        axis_text_y=element_text(size=9),                            # improve y labels
        axis_text_x=element_text(size=6)                             # tidy x labels
    )
    + labs(
        title='Top 15 Features by Total Contribution',
        x='Importance value',
        y='Feature'
    )
)

display(p_imp)

```


```{python}

p_imp = (
    ggplot(plot_long, aes(y='value', x='feature', fill='feature'))
    + geom_col(alpha=0.85)
    + coord_flip()
    + facet_wrap('~metric', scales='free_x', ncol=2)
    + scale_fill_hue()
    + theme_classic()
    + theme(
        legend_position='none',
        axis_text_y=element_text(size=9),   # feature labels
        axis_text_x=element_text(size=9)    # NUMERIC x-axis labels (smaller)
    )
    + labs(
        title='Top 15 Features by Total Contribution',
        x='Importance value',
        y='Feature'
    )
)
display(p_imp)

```

SHAP Values

```{python}

# Create TreeExplainer and compute SHAP values
explainer = shap.TreeExplainer(xgb_tuned)
shap_values = explainer(X_train_enc)

```


```{python}

# SHAP-native summary bar (mean(|SHAP|))
shap.plots.bar(shap_values, max_display=10)

```

```{python}

# Interactive JS init
shap.initjs()

# Beeswarm for binary
shap.plots.beeswarm(shap_values, max_display=25)  # matplotlib figure

```

```{python}

row_idx = 1441 # Set row to look at

# Waterfall
shap.plots.waterfall(shap_values[row_idx], max_display=25)

```

```{python}

row_idx = 2000 # Set row to look at

# Waterfall
shap.plots.waterfall(shap_values[row_idx], max_display=25)

```


```{python}

# Set rows to look at
idx = [1,2,3,4,5]
# Extract base value
base = explainer.expected_value

# Make decision plot
shap.decision_plot(
    base_value=base, # Set base values
    shap_values=shap_values.values[idx, :], # Set SHAP values to use
    features=X_train_enc.iloc[idx, :], # Set data
    feature_names=feature_cols, # Set variable names
    show=True
)

```

```{python}

# SHAP heatmap: cluster cases by similar explanation patterns
shap.plots.heatmap(shap_values[:200], max_display=25)

```